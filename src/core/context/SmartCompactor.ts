/**
 * SmartCompactor - LLM-powered intelligent context compaction
 *
 * Unlike traditional compaction that blindly truncates or removes messages,
 * SmartCompactor uses an LLM to intelligently:
 * - Summarize older conversation segments
 * - Spill large data blobs to working memory
 * - Remove low-value exchanges (acknowledgments, greetings)
 * - Preserve critical context and recent messages
 *
 * This is Phase 4 of the Context Management Balance Fix Plan.
 */

import type { ITextProvider } from '../../domain/interfaces/ITextProvider.js';
import type { WorkingMemory } from '../../capabilities/taskAgent/WorkingMemory.js';
import type { ContextBudget, IContextComponent, PreparedContext } from './types.js';
import { STRATEGY_THRESHOLDS, type StrategyName } from '../constants.js';
import { logger as baseLogger, FrameworkLogger } from '../../infrastructure/observability/Logger.js';

const logger: FrameworkLogger = baseLogger.child({ component: 'SmartCompactor' });

// ============================================================================
// Types
// ============================================================================

/**
 * Configuration for SmartCompactor
 */
export interface SmartCompactorConfig {
  /** Strategy name for threshold lookup */
  strategy: StrategyName;

  /** Maximum context tokens for the model */
  maxContextTokens: number;

  /** Model to use for compaction analysis (uses same provider as agent) */
  model?: string;

  /** Maximum tokens for compaction analysis call */
  maxAnalysisTokens?: number;

  /** Whether to spill data to memory (requires memory to be enabled) */
  enableSpillToMemory?: boolean;

  /** Minimum message count to attempt compaction on */
  minMessagesToCompact?: number;
}

/**
 * Summary generated by smart compaction
 */
export interface CompactionSummary {
  /** Key for the summary (if stored in memory) */
  key?: string;
  /** The summary text */
  summary: string;
  /** Message IDs that were summarized */
  messageIds: string[];
  /** Importance level */
  importance: 'high' | 'medium' | 'low';
}

/**
 * Data spilled to memory
 */
export interface SpilledData {
  /** Memory key where data was stored */
  key: string;
  /** Original message ID */
  messageId: string;
  /** Reason for spilling */
  reason: string;
}

/**
 * Result of smart compaction
 */
export interface SmartCompactionResult {
  /** Summaries created */
  summaries: CompactionSummary[];
  /** Data spilled to memory */
  spilled: SpilledData[];
  /** Message IDs removed */
  removed: string[];
  /** Tokens freed by compaction */
  tokensFreed: number;
  /** Compaction log entries */
  log: string[];
  /** Whether compaction was successful */
  success: boolean;
  /** Error message if failed */
  error?: string;
}

/**
 * LLM analysis result for compaction decisions
 */
interface CompactionDecisions {
  summaries: Array<{
    messageIds: string[];
    summary: string;
    importance: 'high' | 'medium' | 'low';
  }>;
  spill_to_memory: Array<{
    messageId: string;
    key: string;
    reason: string;
  }>;
  safe_to_remove: string[];
  must_keep: string[];
  reasoning: string;
}

// ============================================================================
// SmartCompactor Class
// ============================================================================

/**
 * SmartCompactor - Uses LLM to intelligently compact context
 *
 * Key features:
 * - Strategy-aware thresholds
 * - Preserves recent messages (protected by strategy)
 * - Creates summaries of older conversation segments
 * - Spills large data to WorkingMemory
 * - Removes low-value messages
 */
export class SmartCompactor {
  private readonly config: Required<SmartCompactorConfig>;

  constructor(
    private readonly provider: ITextProvider,
    private readonly memory: WorkingMemory | null,
    config: SmartCompactorConfig
  ) {
    this.config = {
      strategy: config.strategy,
      maxContextTokens: config.maxContextTokens,
      model: config.model ?? 'gpt-4o-mini', // Use a fast model for compaction
      maxAnalysisTokens: config.maxAnalysisTokens ?? 2000,
      enableSpillToMemory: config.enableSpillToMemory ?? (memory !== null),
      minMessagesToCompact: config.minMessagesToCompact ?? 10,
    };
  }

  /**
   * Check if smart compaction should trigger based on current context usage
   */
  shouldTrigger(currentTokens: number): boolean {
    const thresholds = STRATEGY_THRESHOLDS[this.config.strategy] ?? STRATEGY_THRESHOLDS.proactive;
    const triggerAt = this.config.maxContextTokens * thresholds.smartCompactionTrigger;
    return currentTokens >= triggerAt;
  }

  /**
   * Get target token count after compaction (strategy-dependent)
   */
  getTargetTokens(): number {
    const thresholds = STRATEGY_THRESHOLDS[this.config.strategy] ?? STRATEGY_THRESHOLDS.proactive;
    return Math.floor(this.config.maxContextTokens * thresholds.compactionTarget);
  }

  /**
   * Get the number of protected messages based on strategy
   */
  getProtectedMessageCount(): number {
    const thresholds = STRATEGY_THRESHOLDS[this.config.strategy] ?? STRATEGY_THRESHOLDS.proactive;
    // Estimate ~100 tokens per message on average
    const estimatedProtected = Math.floor(
      (this.config.maxContextTokens * thresholds.protectedContextPercent) / 100
    );
    return Math.max(estimatedProtected, 10); // At least 10 messages
  }

  /**
   * Run smart compaction on the provided context
   *
   * @param context - The prepared context to compact
   * @param targetReduction - Optional target reduction percentage (0-100)
   */
  async compact(
    context: PreparedContext,
    targetReduction?: number
  ): Promise<SmartCompactionResult> {
    const log: string[] = [];
    log.push(`[SmartCompactor] Starting compaction with strategy: ${this.config.strategy}`);

    // Extract messages from context components
    const conversationComponent = context.components.find(
      c => c.name === 'conversation_history' || c.name === 'conversation'
    );

    if (!conversationComponent) {
      log.push('[SmartCompactor] No conversation component found - skipping');
      return {
        summaries: [],
        spilled: [],
        removed: [],
        tokensFreed: 0,
        log,
        success: true,
      };
    }

    // Parse messages from component
    const messages = this.parseMessages(conversationComponent);
    if (messages.length < this.config.minMessagesToCompact) {
      log.push(`[SmartCompactor] Only ${messages.length} messages, min ${this.config.minMessagesToCompact} required`);
      return {
        summaries: [],
        spilled: [],
        removed: [],
        tokensFreed: 0,
        log,
        success: true,
      };
    }

    // Calculate target
    const targetTokens = targetReduction
      ? Math.floor(context.budget.used * (1 - targetReduction / 100))
      : this.getTargetTokens();
    const tokensToFree = context.budget.used - targetTokens;

    if (tokensToFree <= 0) {
      log.push('[SmartCompactor] No tokens need to be freed');
      return {
        summaries: [],
        spilled: [],
        removed: [],
        tokensFreed: 0,
        log,
        success: true,
      };
    }

    log.push(`[SmartCompactor] Target: free ${tokensToFree} tokens (${Math.round(tokensToFree / context.budget.used * 100)}%)`);

    try {
      // Build analysis prompt
      const analysisPrompt = this.buildAnalysisPrompt(
        messages,
        context.budget,
        targetTokens
      );

      // Ask LLM to analyze and decide
      const response = await this.provider.generate({
        model: this.config.model,
        input: analysisPrompt,
        max_output_tokens: this.config.maxAnalysisTokens,
        response_format: { type: 'json_object' },
        temperature: 0.3, // More deterministic for analysis
      });

      // Parse decisions - extract text from response
      const responseText = this.extractTextFromResponse(response.output);
      const decisions = this.parseDecisions(responseText);
      log.push(`[SmartCompactor] LLM reasoning: ${decisions.reasoning}`);

      // Execute decisions
      return await this.executeDecisions(decisions, messages, log);
    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : String(error);
      log.push(`[SmartCompactor] Error: ${errorMsg}`);
      logger.error({ error: errorMsg }, 'Smart compaction failed');

      return {
        summaries: [],
        spilled: [],
        removed: [],
        tokensFreed: 0,
        log,
        success: false,
        error: errorMsg,
      };
    }
  }

  /**
   * Parse messages from a context component
   */
  private parseMessages(component: IContextComponent): Array<{
    id: string;
    role: string;
    content: string;
    timestamp?: number;
    tokens?: number;
  }> {
    const content = component.content;

    // If it's a string, try to parse as JSON
    if (typeof content === 'string') {
      try {
        const parsed = JSON.parse(content);
        if (Array.isArray(parsed)) {
          return parsed.map((m, i) => ({
            id: m.id || `msg_${i}`,
            role: m.role || 'unknown',
            content: typeof m.content === 'string' ? m.content : JSON.stringify(m.content),
            timestamp: m.timestamp,
            tokens: m.tokens,
          }));
        }
      } catch {
        // Not JSON, treat as single message
        return [{
          id: 'msg_0',
          role: 'unknown',
          content,
        }];
      }
    }

    // If it's already an array
    if (Array.isArray(content)) {
      return content.map((m, i) => ({
        id: m.id || `msg_${i}`,
        role: m.role || 'unknown',
        content: typeof m.content === 'string' ? m.content : JSON.stringify(m.content),
        timestamp: m.timestamp,
        tokens: m.tokens,
      }));
    }

    return [];
  }

  /**
   * Build the analysis prompt for LLM decision-making
   */
  private buildAnalysisPrompt(
    messages: Array<{ id: string; role: string; content: string; tokens?: number }>,
    budget: ContextBudget,
    targetTokens: number
  ): string {
    const thresholds = STRATEGY_THRESHOLDS[this.config.strategy] ?? STRATEGY_THRESHOLDS.proactive;
    const protectedPercent = Math.round(thresholds.protectedContextPercent * 100);
    const protectedCount = this.getProtectedMessageCount();

    // Summarize messages for the prompt (truncate long content)
    const messageSummaries = messages.map((m) => {
      const truncatedContent = m.content.length > 500
        ? m.content.slice(0, 500) + '...[truncated]'
        : m.content;
      return `[${m.id}] ${m.role}: ${truncatedContent}`;
    });

    return `You are a context compaction assistant. Analyze this conversation and decide how to reduce context size while preserving important information.

## Current State
- Current tokens: ${budget.used}
- Target tokens: ${targetTokens} (${Math.round(targetTokens / this.config.maxContextTokens * 100)}% of max)
- Need to free: ${budget.used - targetTokens} tokens
- Strategy: ${this.config.strategy}
- Protected messages: last ${protectedCount} messages (~${protectedPercent}% of context)

## Messages (${messages.length} total)
${messageSummaries.join('\n\n')}

## Your Task
Respond with JSON only:

{
  "summaries": [
    { "messageIds": ["id1", "id2"], "summary": "Concise summary of these messages", "importance": "high|medium|low" }
  ],
  "spill_to_memory": [
    { "messageId": "id3", "key": "findings.topic_name", "reason": "Large data block that can be retrieved later" }
  ],
  "safe_to_remove": ["id4", "id5"],
  "must_keep": ["id6", "id7"],
  "reasoning": "Brief explanation of your decisions"
}

## Guidelines
- ALWAYS keep the last ${protectedCount} messages (they are protected)
- Summarize older conversation segments that established context
- Spill large data blobs (tool outputs, code blocks >500 chars) to memory with descriptive keys
- Remove acknowledgments ("Got it", "Sure", "OK"), greetings, and low-value exchanges
- NEVER remove system prompts, current task context, or recent tool results
- When summarizing, preserve: key decisions, important findings, action items
- Use meaningful memory keys like "findings.api_research" or "summary.user_requirements"`;
  }

  /**
   * Parse LLM response into compaction decisions
   */
  private parseDecisions(output: string): CompactionDecisions {
    try {
      // Try to extract JSON from the output
      const jsonMatch = output.match(/\{[\s\S]*\}/);
      if (!jsonMatch) {
        throw new Error('No JSON found in response');
      }

      const parsed = JSON.parse(jsonMatch[0]);

      return {
        summaries: parsed.summaries || [],
        spill_to_memory: parsed.spill_to_memory || [],
        safe_to_remove: parsed.safe_to_remove || [],
        must_keep: parsed.must_keep || [],
        reasoning: parsed.reasoning || 'No reasoning provided',
      };
    } catch (error) {
      logger.warn({ error, output }, 'Failed to parse compaction decisions');
      return {
        summaries: [],
        spill_to_memory: [],
        safe_to_remove: [],
        must_keep: [],
        reasoning: 'Failed to parse LLM response',
      };
    }
  }

  /**
   * Execute the compaction decisions
   */
  private async executeDecisions(
    decisions: CompactionDecisions,
    messages: Array<{ id: string; role: string; content: string; tokens?: number }>,
    log: string[]
  ): Promise<SmartCompactionResult> {
    const summaries: CompactionSummary[] = [];
    const spilled: SpilledData[] = [];
    const removed: string[] = [];
    let tokensFreed = 0;

    // Create a map for quick message lookup
    const messageMap = new Map(messages.map(m => [m.id, m]));

    // Process summaries
    for (const summaryDecision of decisions.summaries) {
      let summaryTokens = 0;
      for (const msgId of summaryDecision.messageIds) {
        const msg = messageMap.get(msgId);
        if (msg) {
          summaryTokens += msg.tokens ?? Math.ceil(msg.content.length / 4);
        }
      }

      // Store summary in memory if available
      let summaryKey: string | undefined;
      if (this.memory && this.config.enableSpillToMemory) {
        summaryKey = `summary.compacted_${Date.now()}`;
        try {
          await this.memory.store(
            summaryKey,
            `Summary of ${summaryDecision.messageIds.length} messages`,
            summaryDecision.summary,
            { priority: summaryDecision.importance === 'high' ? 'high' : 'normal' }
          );
          log.push(`[SmartCompactor] Stored summary at ${summaryKey}`);
        } catch (error) {
          logger.warn({ error, key: summaryKey }, 'Failed to store summary in memory');
        }
      }

      summaries.push({
        key: summaryKey,
        summary: summaryDecision.summary,
        messageIds: summaryDecision.messageIds,
        importance: summaryDecision.importance,
      });

      // Summary is much smaller than original
      const summarySize = Math.ceil(summaryDecision.summary.length / 4);
      tokensFreed += Math.max(0, summaryTokens - summarySize);
    }

    // Process spill to memory
    for (const spillDecision of decisions.spill_to_memory) {
      if (!this.memory || !this.config.enableSpillToMemory) {
        log.push(`[SmartCompactor] Memory not available, skipping spill for ${spillDecision.messageId}`);
        continue;
      }

      const msg = messageMap.get(spillDecision.messageId);
      if (!msg) {
        continue;
      }

      try {
        await this.memory.store(
          spillDecision.key,
          spillDecision.reason,
          msg.content,
          { priority: 'normal' }
        );

        spilled.push({
          key: spillDecision.key,
          messageId: spillDecision.messageId,
          reason: spillDecision.reason,
        });

        const msgTokens = msg.tokens ?? Math.ceil(msg.content.length / 4);
        tokensFreed += msgTokens;
        log.push(`[SmartCompactor] Spilled ${spillDecision.messageId} to ${spillDecision.key} (${msgTokens} tokens)`);
      } catch (error) {
        logger.warn({ error, key: spillDecision.key }, 'Failed to spill to memory');
      }
    }

    // Process removals
    for (const msgId of decisions.safe_to_remove) {
      const msg = messageMap.get(msgId);
      if (msg) {
        removed.push(msgId);
        const msgTokens = msg.tokens ?? Math.ceil(msg.content.length / 4);
        tokensFreed += msgTokens;
      }
    }

    log.push(`[SmartCompactor] Completed: ${summaries.length} summaries, ${spilled.length} spilled, ${removed.length} removed, ${tokensFreed} tokens freed`);

    return {
      summaries,
      spilled,
      removed,
      tokensFreed,
      log,
      success: true,
    };
  }

  /**
   * Get compactor configuration
   */
  getConfig(): Readonly<Required<SmartCompactorConfig>> {
    return this.config;
  }

  /**
   * Extract text content from LLM response output
   */
  private extractTextFromResponse(output: unknown): string {
    // Handle string output directly
    if (typeof output === 'string') {
      return output;
    }

    // Handle OutputItem[] (array of content items)
    if (Array.isArray(output)) {
      const texts: string[] = [];
      for (const item of output) {
        if (typeof item === 'string') {
          texts.push(item);
        } else if (item && typeof item === 'object') {
          // Handle different content types
          if ('text' in item && typeof item.text === 'string') {
            texts.push(item.text);
          } else if ('content' in item && typeof item.content === 'string') {
            texts.push(item.content);
          }
        }
      }
      return texts.join('\n');
    }

    // Fallback: stringify the object
    return JSON.stringify(output);
  }
}

// ============================================================================
// Factory Function
// ============================================================================

/**
 * Create a SmartCompactor instance
 */
export function createSmartCompactor(
  provider: ITextProvider,
  memory: WorkingMemory | null,
  config: SmartCompactorConfig
): SmartCompactor {
  return new SmartCompactor(provider, memory, config);
}
